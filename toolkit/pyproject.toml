[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "histtext_toolkit"
version = "1.1.0"
description = "A toolkit for working with Apache Solr, including enhanced NER, tokenization, and embeddings operations with state-of-the-art models"
readme = "README.md"
requires-python = ">=3.8"
license = {text = "MIT"}
authors = [
    {name = "Baptiste Blouin", email = "histtext@gmail.com"}
]
keywords = [
    "nlp", "ner", "solr", "text-processing", "historical-text", 
    "gliner", "transformers", "embeddings", "tokenization", "multilingual"
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Text Processing :: Linguistic",
    "Topic :: Software Development :: Libraries :: Python Modules",
]

dependencies = [
    # Core dependencies
    "aiohttp>=3.8.0",
    "jsonlines>=2.0.0",
    "pyyaml>=6.0",
    "tqdm>=4.62.0",
    "numpy>=1.20.0",
    
    # CLI and configuration
    "click>=8.0.0",
    "pydantic>=2.0.0",
    
    # Core ML dependencies - using flexible versions
    "torch>=1.12.0",  # More flexible to avoid conflicts
    "transformers>=4.21.0",  # Compatible with older torch
    "datasets>=2.0.0",
    
    # Additional core packages
    "psutil>=5.8.0",
    "scikit-learn>=1.0.0",
]

[project.optional-dependencies]
# Traditional NLP models
spacy = [
    "spacy>=3.4.0",
]

# Core transformers models
transformers = [
    "transformers>=4.21.0",
    "torch>=1.12.0",
]

# Modern NER models (PyTorch 2.0+ compatible)
modern_ner = [
    "torch>=2.0.0",
    "transformers>=4.35.0",
    "accelerate>=0.24.0",
    "gliner>=0.2.0",
    "flair>=0.12.0",
    "optimum>=1.14.0",
]

# Enhanced NER models (backward compatible)
enhanced_ner = [
    "gliner>=0.2.0",
    "flair>=0.12.0",
    "optimum>=1.14.0",
    "stanza>=1.4.0",
]

# Individual model frameworks
gliner = [
    "gliner>=0.2.0",
]

flair = [
    "flair>=0.12.0",
]

nuner = [
    "gliner>=0.2.0",  # NuNER is GLiNER-based
]

stanza = [
    "stanza>=1.4.0",
]

# AllenNLP - separate due to PyTorch conflicts
allennlp = [
    "torch>=1.10.0,<1.13.0",  # AllenNLP's requirement
    "allennlp>=2.10.0",
    "allennlp-models>=2.10.0",
]

# Chinese NLP frameworks
chinese_basic = [
    "hanziconv>=0.3.2",
    "jieba>=0.42.0",
]

fasthan = [
    "fasthan>=1.5.0",
    "torch>=1.10.0",
]

fastnlp = [
    "fastnlp>=0.7.0",
    "torch>=1.10.0",
]

lac = [
    "lac>=2.1.0",  # Baidu LAC
]

hanlp = [
    "hanlp>=2.1.0",
]

pkuseg = [
    "pkuseg>=0.0.25",
]

# Comprehensive Chinese NLP
chinese = [
    "hanziconv>=0.3.2",
    "jieba>=0.42.0",
    "fasthan>=1.5.0",
    "fastnlp>=0.7.0",
    "lac>=2.1.0",
    "hanlp>=2.1.0",
    "pkuseg>=0.0.25",
]

# Chinese word segmentation
chinese_segmenter = [
    "hanziconv>=0.3.2",
    "jieba>=0.42.0",
]

# Multilingual and language detection
multilingual = [
    "langdetect>=1.0.9",
    "polyglot>=16.07.04",
]

# Auto multilingual model selection
auto_multilingual = [
    "langdetect>=1.0.9",
    "transformers>=4.21.0",
    "torch>=1.12.0",
]

# Historical text processing
historical = [
    "transformers>=4.21.0",
    "torch>=1.12.0",
    "langdetect>=1.0.9",
]

# Embeddings
fasttext = [
    "fasttext>=0.9.2",
]

word2vec = [
    "gensim>=4.0.0",
]

sentence_transformers = [
    "sentence-transformers>=2.0.0",
]

word_embeddings = [
    "gensim>=4.0.0",
    "nltk>=3.6.0",
]

embeddings = [
    "fasttext>=0.9.2",
    "gensim>=4.0.0",
    "sentence-transformers>=2.0.0",
    "nltk>=3.6.0",
]

# LLM-based NER (requires modern PyTorch)
llm_ner = [
    "torch>=2.0.0",
    "transformers>=4.35.0",
    "accelerate>=0.24.0",
    "bitsandbytes>=0.41.0",
]

# Specific LLM models
deepseek = [
    "torch>=2.0.0",
    "transformers>=4.35.0",
    "accelerate>=0.24.0",
]

phi3 = [
    "torch>=2.0.0",
    "transformers>=4.35.0",
    "accelerate>=0.24.0",
]

qwen = [
    "torch>=2.0.0",
    "transformers>=4.35.0",
    "accelerate>=0.24.0",
]

mistral = [
    "torch>=2.0.0",
    "transformers>=4.35.0",
    "accelerate>=0.24.0",
]

# Performance optimizations
performance = [
    "optimum>=1.14.0",
    "psutil>=5.8.0",
]

# Development dependencies
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.20.0",
    "pytest-cov>=4.0.0",
    "black>=22.0.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
    "pre-commit>=2.20.0",
]

# Testing
test = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.20.0",
    "pytest-cov>=4.0.0",
    "pytest-mock>=3.10.0",
]

# Documentation
docs = [
    "sphinx>=5.0.0",
    "sphinx-rtd-theme>=1.2.0",
    "myst-parser>=0.18.0",
    "sphinx-autodoc-typehints>=1.19.0",
]

# Essential features (most common use cases, no conflicts)
essential = [
    # Traditional NLP
    "spacy>=3.4.0",
    
    # Modern NER (without AllenNLP)
    "gliner>=0.2.0",
    "flair>=0.12.0",
    "optimum>=1.14.0",
    "stanza>=1.4.0",
    
    # Basic Chinese processing
    "hanziconv>=0.3.2",
    "jieba>=0.42.0",
    
    # Embeddings
    "fasttext>=0.9.2",
    "gensim>=4.0.0",
    "sentence-transformers>=2.0.0",
    "nltk>=3.6.0",
    
    # Multilingual support
    "langdetect>=1.0.9",
    
    # Performance
    "psutil>=5.8.0",
    "optimum>=1.14.0",
]

# Comprehensive Chinese NLP suite
chinese_full = [
    # Basic Chinese
    "hanziconv>=0.3.2",
    "jieba>=0.42.0",
    
    # Advanced Chinese NLP
    "fasthan>=1.5.0",
    "fastnlp>=0.7.0",
    "lac>=2.1.0",
    "hanlp>=2.1.0",
    "pkuseg>=0.0.25",
    
    # PyTorch support for Chinese models
    "torch>=1.12.0",
]

# All transformers-based models (multilingual)
transformers_full = [
    "torch>=1.12.0",
    "transformers>=4.21.0",
    "accelerate>=0.20.0",
    "datasets>=2.0.0",
    "optimum>=1.14.0",
    "langdetect>=1.0.9",
]

# Complete installation (with modern PyTorch, most features)
all = [
    # Traditional NLP
    "spacy>=3.4.0",
    
    # Modern NER (with PyTorch 2.0+)
    "torch>=2.0.0",
    "transformers>=4.35.0",
    "accelerate>=0.24.0",
    "gliner>=0.2.0",
    "flair>=0.12.0",
    "optimum>=1.14.0",
    "stanza>=1.4.0",
    
    # Chinese processing (all frameworks)
    "hanziconv>=0.3.2",
    "jieba>=0.42.0",
    "fasthan>=1.5.0",
    "fastnlp>=0.7.0",
    "lac>=2.1.0",
    "hanlp>=2.1.0",
    "pkuseg>=0.0.25",
    
    # Embeddings
    "fasttext>=0.9.2",
    "gensim>=4.0.0",
    "sentence-transformers>=2.0.0",
    "nltk>=3.6.0",
    
    # LLM support
    "bitsandbytes>=0.41.0",
    
    # Multilingual support
    "langdetect>=1.0.9",
    
    # Performance
    "psutil>=5.8.0",
    "optimum>=1.14.0",
]

# Legacy support (for older PyTorch environments with AllenNLP)
legacy = [
    "torch>=1.10.0,<1.13.0",
    "transformers>=4.21.0,<4.35.0",
    "allennlp>=2.10.0",
    "allennlp-models>=2.10.0",
    "spacy>=3.4.0",
    "stanza>=1.4.0",
]

# Compatibility bundles for specific use cases
research = [
    # Research-grade models
    "torch>=2.0.0",
    "transformers>=4.35.0",
    "accelerate>=0.24.0",
    "gliner>=0.2.0",
    "flair>=0.12.0",
    "stanza>=1.4.0",
    "optimum>=1.14.0",
    "bitsandbytes>=0.41.0",
]

production = [
    # Production-ready, stable models
    "spacy>=3.4.0",
    "transformers>=4.21.0",
    "torch>=1.12.0",
    "stanza>=1.4.0",
    "fasttext>=0.9.2",
    "sentence-transformers>=2.0.0",
]

[project.scripts]
histtext-toolkit = "histtext_toolkit.cli:main_cli"
histtext = "histtext_toolkit.cli:main_cli"

[project.urls]
Homepage = "https://github.com/BaptisteBlouin/HistText"
Repository = "https://github.com/BaptisteBlouin/HistText"
Documentation = "https://github.com/BaptisteBlouin/HistText/wiki"
"Bug Reports" = "https://github.com/BaptisteBlouin/HistText/issues"

[tool.setuptools.packages.find]
where = ["."]
include = ["histtext_toolkit*"]

[tool.setuptools.package-data]
histtext_toolkit = ["py.typed"]

[tool.black]
line-length = 150
target-version = ["py310"]
include = '\.pyi?$'

[tool.ruff]
line-length = 150
exclude = [
    ".git",
    "__pycache__",
    "cache",
    "build",
    "dist",
]

[tool.ruff.lint]
select = ["E", "F", "I", "N", "B", "UP", "W", "D"]
ignore = ["D203", "D213"]
fixable = ["ALL"]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"test_*.py" = ["D"]

[tool.ruff.lint.isort]
known-first-party = ["histtext_toolkit"]

[tool.mypy]
python_version = "3.12"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--cov=histtext_toolkit",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-report=xml",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
]

[tool.coverage.run]
source = ["histtext_toolkit"]
omit = [
    "*/tests/*",
    "*/test_*",
    "*/__init__.py",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
]
